{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必需的库\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载自定义的工具类函数\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "# 设置一些绘图相关的参数\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# 固定随机数种子\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/dims1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数函数\n",
    "# layer_dims 包含每一层的神经元个数 (输入层、第一层、第二层... 输出层)\n",
    "# 如 [5, 4, 3] 表示输入层有 5 个神经元，第一层有 4 个神经元，最后一层有 3 个神经元\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    # 固定随机数种子\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # 初始化好的参数\n",
    "    parameters = {}\n",
    "\n",
    "    # 获取神经网络层数 (包含了输入层)\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    # 跳过了输入层\n",
    "    for l in range(1, L):\n",
    "        # Wl 的维度是 (n[l], n[l-1])\n",
    "        # np.sqrt(layer_dims[l-1])  为了延缓梯度消失和爆炸，我们需要将权重初始化为更靠近0的数\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "\n",
    "        # 构建并初始化 b\n",
    "        # bl 的维度 (n[l], 1)\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        # 核对 w, b 的维度\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`linear_forward` 用于实现公式 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$， 称之为线性前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性前向传播\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "    # 核对 Z 的维度\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "\n",
    "    # 保存 A, W, b\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`linear_activation_forward` 用于实现公式 $A^{[l]} = g(Z^{[l]})$，g代表激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非线性前向传播\n",
    "# A_prev 上一层的 A\n",
    "# W 本层的 w\n",
    "# b 本层的 b\n",
    "# activation 激活函数, 'sigmoid'/'relu'\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    # 计算本层的 Z\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    # 使用激活函数\n",
    "    if activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A = relu(Z)\n",
    "\n",
    "    # 核对 A 的维度\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "\n",
    "    # 缓存 Z, linear_cache\n",
    "    cache = (linear_cache, Z)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/forward_propagation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播\n",
    "# 前 L-1 层使用 relu 激活函数，最后一层使用 sigmoid 激活函数\n",
    "# 输入特征数据 X，每一层的 w, b (parameters)\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "\n",
    "    # 第一层的 A 实际就是输入的特征 X\n",
    "    A = X\n",
    "\n",
    "    # 获取神经网络层数\n",
    "    # 注意：此处的层数不包括输入层\n",
    "    # 参数列表数据为 [W1, b1, W2, b2, ...]，一对 (W1, b1) 即为一层\n",
    "    # 因此除以 2 可以得到神经网络的层数\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    # 先进行前 L-1 层的前向传播\n",
    "    # 使用 relu 激活函数\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], \n",
    "                                            parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # 进行最后移一层的前向传播\n",
    "    # 使用 sigmoid 激活函数\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)],\n",
    "                                            parameters['b' + str(L)], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    # 核算 AL 的维度\n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 成本计算\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "\n",
    "    # 确保 cost 是数值形式不是数组形式\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`linear_backward` 函数用于根据后一层的 dZ 来计算前面一层的 dW，db 和 dA。也就是实现了下面3个公式\n",
    "$$ dW^{[l]}  = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]}  = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "# dZ 后一层的dZ\n",
    "# cache 本层前向传播时的缓存变量\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    # 取出本层前向传播时的 A_prev, W, b\n",
    "    A_prev, W, b = cache\n",
    "\n",
    "    # 样本数\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    # 按上述公式计算 dW, db, dA_prev\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    # 核算维度\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`linear_activation_backward` 用于根据本层的 dA 计算出本层的 dZ。就是实现了下面的公式\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非线性反向传播\n",
    "# dA 本层的 dA\n",
    "# cache 本层在前向传播时保存的变量\n",
    "# activation 激活函数，'sigmoid'/'relu\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    # 取出变量\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    # 根据本层的 dZ 计算本层的 dW, db, 以及前一层的 dA\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/backward_propagation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的反向传播\n",
    "# AL 最后一层的 A，即 y'，预测出的标签\n",
    "# Y 真实标签\n",
    "# caches 每层的缓存\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "\n",
    "    # 神经网络层数\n",
    "    L = len(caches)\n",
    "\n",
    "    # 让真实标签和预测标签的维度一致\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    # 根据成本函数直接求 dA\n",
    "    # 最后一层的 dA 与前面各层的 dA 计算方式不同\n",
    "    dAL =  -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "\n",
    "    # 单独计算最后一层的 dW 和 db，使用了 sigmoid 函数\n",
    "    current_cache = caches[-1]\n",
    "    grads['dA' + str(L-1)], \\\n",
    "        grads['dW' + str(L)], \\\n",
    "            grads['db' + str(L)] = linear_activation_backward(dAL, current_cache, activation='sigmoid')\n",
    "\n",
    "    # 计算 L-1 层到第一层的梯度，使用 relu 函数\n",
    "    # reversed: L-1, L-2, L-3 ... 1\n",
    "    for c in reversed(range(1, L)):\n",
    "        grads['dA' + str(c-1)], \\\n",
    "            grads['dW' + str(c)], \\\n",
    "                grads['db' + str(c)] = linear_activation_backward(grads['dA' + str(c)], caches[c-1], 'relu')\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`update_parameters` 函数将利用这些梯度来更新/优化每一层的w和b，也就是进行梯度下降。\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降\n",
    "# parameters: 每层的 w, b\n",
    "# grads: 每层的梯度即dW, db\n",
    "# learning_rate: 学习率\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    # 获取层数\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        parameters['W' + str(l)] = parameters['W' + str(l)] - learning_rate * grads['dW' + str(l)]\n",
    "        parameters['b' + str(l)] = parameters['b' + str(l)] - learning_rate * grads['db' + str(l)]\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具函数编写完成\n",
    "# 接下来加载数据\n",
    "\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "m_train = train_x_orig.shape[0]  # 训练样本的数量\n",
    "m_test = test_x_orig.shape[0]  # 测试样本的数量\n",
    "num_px = test_x_orig.shape[1]  # 每张图片的宽/高\n",
    "\n",
    "# 为了方便后面进行矩阵运算，我们需要将样本数据进行扁平化和转置\n",
    "# 处理后的数组各维度的含义是（图片数据，样本数）\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T \n",
    "\n",
    "# 下面我们对特征数据进行了简单的标准化处理（除以255，使所有值都在[0，1]范围内）\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度神经网络训练模型\n",
    "# X 数据特征\n",
    "# Y 数据标签\n",
    "# layers_dims 神经网络每层的数量 (包含输出层)\n",
    "# learning_rate 学习率\n",
    "# num_iterations 训练次数\n",
    "\n",
    "def dnn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    # 固定随机数种子\n",
    "    np.random.seed(1)\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    # 初始化每层的 w, b\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # 训练指定次数\n",
    "    for i in range(num_iterations):\n",
    "        # 前向传播\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        # 计算成本\n",
    "        cost = compute_cost(AL, Y)\n",
    "        # 反向传播\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # 更新参数\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        # 每 100 次打印一次成本\n",
    "        if i % 100 == 0:\n",
    "            if print_cost and i > 0:\n",
    "                print (\"训练%i次后成本是: %f\" % (i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    # 画出成本曲线图\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练100次后成本是: 0.672053\n",
      "训练200次后成本是: 0.648263\n",
      "训练300次后成本是: 0.611507\n",
      "训练400次后成本是: 0.567047\n",
      "训练500次后成本是: 0.540138\n",
      "训练600次后成本是: 0.527930\n",
      "训练700次后成本是: 0.465477\n",
      "训练800次后成本是: 0.369126\n",
      "训练900次后成本是: 0.391747\n",
      "训练1000次后成本是: 0.315187\n",
      "训练1100次后成本是: 0.272700\n",
      "训练1200次后成本是: 0.237419\n",
      "训练1300次后成本是: 0.199601\n",
      "训练1400次后成本是: 0.189263\n",
      "训练1500次后成本是: 0.161189\n",
      "训练1600次后成本是: 0.148214\n",
      "训练1700次后成本是: 0.137775\n",
      "训练1800次后成本是: 0.129740\n",
      "训练1900次后成本是: 0.121225\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置好深度神经网络的层次信息——下面代表了一个4层的神经网络（12288是输入层），\n",
    "# 第一层有20个神经元，第二层有7个神经元...\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# 根据上面的层次信息来构建一个深度神经网络，并且用之前加载的数据集来训练这个神经网络，得出训练后的参数\n",
    "parameters = dnn_model(train_x, train_y, layers_dims, num_iterations=2000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "\n",
    "def predict(X, parameters):\n",
    "    # 样本数量\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # 神经网络层数 (包括输出层)\n",
    "    n = len(parameters) // 2\n",
    "\n",
    "    # 结果\n",
    "    p = np.zeros((1, m))\n",
    "\n",
    "    # 进行一次前向传播得到预测结果\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # 将预测结果转换为 0/1\n",
    "    # 大于 0.5 为 1，反之为 0\n",
    "    for i in range(probas.shape[1]):\n",
    "        if (probas[0, i] > 0.5):\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率是: 0.9808612440191385\n"
     ]
    }
   ],
   "source": [
    "# 对训练数据集进行预测\n",
    "pred_train = predict(train_x, parameters)\n",
    "print(\"预测准确率是: \" + str(np.sum((pred_train == train_y) / train_x.shape[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率是: 0.8\n"
     ]
    }
   ],
   "source": [
    "# 对测试数据集进行预测\n",
    "pred_test = predict(test_x, parameters)\n",
    "print(\"预测准确率是: \" + str(np.sum((pred_test == test_y) / test_x.shape[1])))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "133e0a373141b86a74d8ce31a3386c60afe8de1148488403c9ad7c02ff4c740c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('directml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
