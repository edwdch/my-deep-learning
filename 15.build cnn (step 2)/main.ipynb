{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入 build cnn (step 1) 中编写的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必需的库\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 零填充\n",
    "def zero_pad(X, pad):\n",
    "    return np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单步卷积\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    s = np.multiply(a_slice_prev, W) + b\n",
    "    return np.sum(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积网络的前向传播\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    stride = hparameters['stride']  \n",
    "    pad = hparameters['pad']       \n",
    "\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]   \n",
    "        for h in range(n_H):            \n",
    "            for w in range(n_W):   \n",
    "                vert_start = h * stride\n",
    "                vert_end = vert_start + f\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end]\n",
    "                for c in range(n_C):   \n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[..., c], b [..., c])\n",
    "    \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.15585932488906465\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "# unit test\n",
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 1}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 池化层\n",
    "\n",
    "池化层可以将矩阵的尺寸变小，降低神经网络的计算量，让网络的预测鲁棒性更好。\n",
    "\n",
    "两种池化层：\n",
    "\n",
    "1. 最大池化层 (Max-pooling layer)：取输入矩阵的子矩阵中最大值元素作为输出矩阵的一个元素\n",
    "2. 平均池化层 (Average-pooling layer)：取输入矩阵的子矩阵中所有元素的平均值作为输出矩阵的一个元素。\n",
    "\n",
    "`子矩阵` 在这里也叫做 `窗口`。\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:230px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:230px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "池化层没有参数，因为它的过滤器是虚拟的不存在，但有超参数，如窗口大小 f，步长 s。\n",
    "\n",
    "## 池化的前向传播\n",
    "\n",
    "计算公式如下：\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    '''\n",
    "    参数：\n",
    "    A_prev -- 输入矩阵，也就是上一层的输出矩阵，维度是 (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- 超参数，窗口大小 f 和步长 s\n",
    "    mode -- 池化模式，最大池传入 'max'，平均池传入 'average'\n",
    "\n",
    "    返回值：\n",
    "    A -- 池化层的输出参数，维度是 (m, n_H, n_W, n_C)\n",
    "    cache -- 缓存一些数据\n",
    "    '''\n",
    "\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    f = hparameters['f']                # 窗口大小\n",
    "    stride = hparameters['stride']      # 步长\n",
    "\n",
    "    # 计算输出矩阵的大小\n",
    "    n_H = int((n_H_prev - f) / stride) + 1\n",
    "    n_W = int((n_W_prev - f) / stride) + 1\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    # 初始化输出矩阵\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    for i in range(m):                  # 遍历所有样本\n",
    "        for h in range(n_H):            # 遍历输出矩阵的高\n",
    "            for w in range(n_W):        # 遍历输出矩阵的宽\n",
    "                # 计算本次池化区域的索引\n",
    "                vert_start = h * stride\n",
    "                vert_end = vert_start + f\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range(n_C):    # 遍历输出矩阵的深度\n",
    "                    # 取出将池化的子矩阵窗口\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                    # 执行池化\n",
    "                    if mode == 'max':\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == 'average':\n",
    "                        A[i, h, w, c] = np.average(a_prev_slice)\n",
    "\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    cache = (A_prev, hparameters)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 1.6924546  2.10025514]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 1.51981682 2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
      "\n",
      "\n",
      " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
     ]
    }
   ],
   "source": [
    "# 单元测试\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 4}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 的反向传播\n",
    "\n",
    "CNN 的反向传播比较复杂，了解即可\n",
    "\n",
    "## 卷积层的反向传播\n",
    "\n",
    "首先我们来学习一下如何实现卷积层的反向传播。\n",
    "\n",
    "### 1. 计算dA:\n",
    "下面的公式被用来计算某个样本的某个过滤器的dA:\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "$W_c$是表示这个过滤器。\n",
    "\n",
    "这个公式对应的python代码如下：\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "### 2. 计算dW:\n",
    "下面的公式被用来计算某个过滤器的dW:\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "$a_{slice}$表示输入矩阵中的被卷积的子矩阵。\n",
    "\n",
    "上面的公式对应于下面的python代码:\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "### 3. 计算db:\n",
    "\n",
    "这个公式用来计算某个过滤器的db:\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "上面的公式对应于下面的python代码:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    '''\n",
    "    参数：\n",
    "    dZ -- 后一层相关的dZ，维度是(m, n_H, n_W, n_C)\n",
    "    cache -- 前面的conv_forward()函数保存下来的缓存数据\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- 本卷积层输入矩阵的dA，维度是(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- 本卷积层相关的dW,维度是(f, f, n_C_prev, n_C)\n",
    "    db -- 本卷积层相关的db,维度是(1, 1, 1, n_C)\n",
    "    '''\n",
    "\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"] # 步长\n",
    "    pad = hparameters[\"pad\"] # padding数量\n",
    "    \n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # 遍历每一个样本\n",
    "        \n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # 遍历输出矩阵的高\n",
    "            for w in range(n_W):               # 遍历输出矩阵的宽\n",
    "                for c in range(n_C):           # 遍历输出矩阵的深度\n",
    "                    \n",
    "                    # 计算输入矩阵中的子矩阵的索引\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # 取出当前进行卷积的子矩阵\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # 用上面的公式来计算偏导数\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.608990675868995\n",
      "dW_mean = 10.581741275547563\n",
      "db_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "# print(dA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化层的反向传播\n",
    "\n",
    "### 1. 最大池化的反向传播\n",
    "\n",
    "在实现池化层之前，我们需要先实现两个工具函数，第一个是 `create_mask_from_window()`，它可以根据输入矩阵得到一个特殊的输出矩阵，这个输出矩阵中只有最大值处是 1，其余都是零。如下所示，X 是输入矩阵，M 是函数的输出矩阵: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "提示:\n",
    "- [np.max()]() 会返回矩阵中的最大元素。\n",
    "- python语法 `A = (X == x)` 会生成一个矩阵 A，这个 A 与 X 的维度是一样的，A 里面其它元素都为 0，只有与小 x 的值相同的位置处为 1，也就是为 True。python 中 0 等于 False，1 等于 True:\n",
    "```\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "\n",
    "    # x是一个矩阵。np.max(x)会得到最大元素。\n",
    "    # mask是一个与x维度相同的矩阵，里面其余元素都为0，只有x最大值元素的位置处为1\n",
    "    mask = x == np.max(x)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 平均池化的反向传播\n",
    "\n",
    "为了实现最大池化的反向传播，我们需要实现如下的工具函数 `distribute_value`。就是把一个数值平分成一个矩阵，例如把 1 平分成四分之一到一个矩阵中: \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"    \n",
    "    参数:\n",
    "    dz -- 一个数值\n",
    "    shape -- 输出矩阵的维度\n",
    "    \n",
    "    返回值:\n",
    "    a -- a的维度就是shape，里面的值是又dz平分而来的\n",
    "    \"\"\"\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # 计算平均值\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # 构建输出矩阵\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    池化层的反向传播\n",
    "\n",
    "    参数:\n",
    "    dA -- 本池化层的输出矩阵对应的偏导数\n",
    "    cache -- 前向传播时缓存起来的数值\n",
    "    mode -- 是最大池化还是平均池化，(\"max\" 或 \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- 本池化层的输入矩阵对应的偏导数\n",
    "    \"\"\"\n",
    "\n",
    "    # A_prev是本池化层的输入矩阵\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                     \n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                  \n",
    "            for w in range(n_W):             \n",
    "                for c in range(n_C):          \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f, f)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "# 单元测试\n",
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94ab98b4b931eea8da1b5885e6fe9f1cfe685315bc68825de5fd10f928659d37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
