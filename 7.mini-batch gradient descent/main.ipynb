{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux/workspace/my-deep-learning/7.mini-batch gradient descent/opt_utils.py:76: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n",
      "/home/linux/workspace/my-deep-learning/7.mini-batch gradient descent/opt_utils.py:77: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n"
     ]
    }
   ],
   "source": [
    "# 加载必需的库\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from testCases import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降中，用于更新参数的函数\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    # 获取神经网络的层数\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    # 遍历每一层\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch 梯度下降和随机梯度下降的差别\n",
    "\n",
    "- **batch梯度下降**:\n",
    "\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(a, Y)\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "```\n",
    "\n",
    "- **随机梯度下降**:\n",
    "\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    for j in range(0, m): # 遍历循环每一个样本\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        cost = compute_cost(a, Y[:,j])\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "左图是随机梯度下降，右图是batch梯度下降\n",
    "\n",
    "![](./images/kiank_sgd.png)\n",
    "\n",
    "左图是随机梯度下降，右图是mini-batch梯度下降\n",
    "\n",
    "![](./images/kiank_minibatch.png)\n",
    "\n",
    "注意\n",
    "\n",
    "- 这3个梯度下降的区别仅仅在于它们每次学习的样本数量不同。\n",
    "- 无论是哪种梯度下降，学习率都是必须要精心调的。\n",
    "- 通常来说，如果数据集很大，那么mini-batch梯度下降会比另外2种要高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch 梯度下降步骤\n",
    "\n",
    "## 1. 洗牌\n",
    "\n",
    "![](./images/kiank_shuffle.png)\n",
    "\n",
    "## 2. 分割\n",
    "\n",
    "![](./images/kiank_partition.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数实现\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n",
    "    # 初始化随机数种子\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "\n",
    "    # 第一步，洗牌\n",
    "    # 生成 m 范围内的随机整数，如 m=3，则结果可能是 [2, 0, 1]\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "    # 第二步，分割\n",
    "    # 获取子训练集的个数（不包括最后一个除不尽的）\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    for k in range(num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size: (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: (k+1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # 最后处理无法除尽的子训练集\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重点\n",
    "\n",
    "- 洗牌和分割是 mini-batch 的两个重要步骤。\n",
    "- mini-batch 的大小选择一般是 2 的次方：16，32，64，128..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "133e0a373141b86a74d8ce31a3386c60afe8de1148488403c9ad7c02ff4c740c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('directml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
